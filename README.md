# clip-loc-maps
This is the repository for the paper "Integrating Visual and Textual Inputs for Searching Large-Scale Map Collections with CLIP," accepted to the 2024 Computational Humanities Research (CHR) conference.

Helper files (those too big for GitHub) can be found at the project's public [Zenodo](https://zenodo.org/records/11538437?preview=1&token=eyJhbGciOiJIUzUxMiJ9.eyJpZCI6IjFmOTY0ZTkxLTI4MTMtNDcwZS1iZDlkLTE3MzI0N2UwZjBhOSIsImRhdGEiOnt9LCJyYW5kb20iOiJmY2I1ZDhiMTdiZjdhZGQ4NGExZmYwYTU0ZWQ5NWEwYyJ9.0UTJ1hiE82QAINiushqIYy5YVmT5Af40XCVJxEc63Eppapa5SK1L_kuGkYx4f_OBQoZ5MHdY2Z27QDyCPXYrbQ). We recommend placing `beto`, `beto_idx`, and `beto_normalized` into the `search` folder for compatibility.

The `embeddings` folder contains scripts that accept resource URLs (in the form specified in `p1_map_file_list.csv` and return CLIP-generated embeddings. `embed.stripped` has functionality to load a model checkpoint for fine-tuning experiments. `create_beto` accepts the JSON files generated by `embed_*` and creates `beto`, `beto_idx`, and `beto_normalized.`

The `fine-tuning` folder contains script for dataset creation, a notebook for fine-tuning incrementally, and the fine-tuning script. Fine-tuning accepts a range of image-text pairs with user-specified model hyperparameters.

Lastly, `search` loads in `beto` and `beto_idx` to accept user-specified search inputs. All preamble cells must be run for the search cells to work. 
